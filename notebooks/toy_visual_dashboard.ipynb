{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toy Experiment Visual Dashboard\n",
        "\n",
        "이 노트북은 `runs/toy` 결과를 모델별로 한눈에 비교하기 위한 시각화 대시보드입니다.\n",
        "- 지원 모델 라벨: `M0~M4`\n",
        "- 레거시 경로(`baseline/reg/struct`)도 자동 인식\n",
        "- 학습 곡선, FID vs NFE, integrability sigma 곡선, 벡터장/컬(curl), 샘플 궤적 비교\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "ROOT = Path('.').resolve()\n",
        "if str(ROOT) not in sys.path:\n",
        "    # 로컬 src 모듈 임포트를 위해 repo 루트를 경로에 추가한다.\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from src.models import build_model, score_fn_from_model\n",
        "from src.sampling import sample_heun\n",
        "from src.utils.checkpoint import latest_checkpoint, load_checkpoint\n",
        "from src.utils.config import ensure_experiment_defaults, resolve_model_id\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "RUN_ROOT = ROOT / 'runs' / 'toy'\n",
        "MODEL_ORDER = ['M0', 'M1', 'M2', 'M3', 'M4']\n",
        "LEGACY_ALIAS = {\n",
        "    'M0': ['M0', 'baseline'],\n",
        "    'M1': ['M1', 'reg'],\n",
        "    'M2': ['M2', 'struct'],\n",
        "    'M3': ['M3'],\n",
        "    'M4': ['M4'],\n",
        "}\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'ROOT={ROOT}')\n",
        "print(f'RUN_ROOT={RUN_ROOT}')\n",
        "print(f'DEVICE={DEVICE}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_toy_run_dirs(run_root: Path, seed_name: str = 'seed0') -> dict[str, Path]:\n",
        "    \"\"\"Resolve model-id to run directory mapping under runs/toy.\n",
        "\n",
        "    Args:\n",
        "        run_root: Root path like `runs/toy`.\n",
        "        seed_name: Seed directory name.\n",
        "\n",
        "    Returns:\n",
        "        Mapping `{model_id: run_dir}` for detected runs.\n",
        "\n",
        "    How it works:\n",
        "        For each model id in M0~M4, tries canonical folder first and\n",
        "        then legacy aliases (baseline/reg/struct).\n",
        "    \"\"\"\n",
        "    out: dict[str, Path] = {}\n",
        "    for model_id in MODEL_ORDER:\n",
        "        for folder in LEGACY_ALIAS.get(model_id, [model_id]):\n",
        "            candidate = run_root / folder / seed_name\n",
        "            if candidate.exists():\n",
        "                out[model_id] = candidate\n",
        "                break\n",
        "    return out\n",
        "\n",
        "\n",
        "def read_csv_if_exists(path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Read CSV file if present, otherwise return empty DataFrame.\n",
        "\n",
        "    Args:\n",
        "        path: CSV path.\n",
        "\n",
        "    Returns:\n",
        "        Loaded DataFrame or empty DataFrame when file is missing.\n",
        "    \"\"\"\n",
        "    if not path.exists():\n",
        "        return pd.DataFrame()\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "\n",
        "def maybe_run_eval(run_dir: Path, nfe_list: str = '10,20,50,100,200') -> None:\n",
        "    \"\"\"Run `src.main_eval` for a run when eval artifacts are missing.\n",
        "\n",
        "    Args:\n",
        "        run_dir: Run directory path.\n",
        "        nfe_list: Comma-separated NFE list string.\n",
        "\n",
        "    Returns:\n",
        "        None. Subprocess call writes eval files in-place.\n",
        "    \"\"\"\n",
        "    fid_path = run_dir / 'eval' / 'fid_vs_nfe.csv'\n",
        "    integ_path = run_dir / 'eval' / 'integrability_vs_sigma.csv'\n",
        "    if fid_path.exists() and integ_path.exists():\n",
        "        return\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable, '-m', 'src.main_eval',\n",
        "        '--run_dir', str(run_dir),\n",
        "        '--nfe_list', nfe_list,\n",
        "    ]\n",
        "    print('Running:', ' '.join(cmd))\n",
        "    subprocess.run(cmd, check=True, cwd=str(ROOT))\n",
        "\n",
        "\n",
        "def load_run_cfg(run_dir: Path) -> dict:\n",
        "    \"\"\"Load resolved run config YAML and normalize experiment defaults.\n",
        "\n",
        "    Args:\n",
        "        run_dir: Run directory containing `config_resolved.yaml`.\n",
        "\n",
        "    Returns:\n",
        "        Resolved config dictionary.\n",
        "    \"\"\"\n",
        "    cfg_path = run_dir / 'config_resolved.yaml'\n",
        "    with cfg_path.open('r', encoding='utf-8') as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    return ensure_experiment_defaults(cfg)\n",
        "\n",
        "\n",
        "def load_model_score_fn(run_dir: Path, device: torch.device):\n",
        "    \"\"\"Build model and score function from latest checkpoint.\n",
        "\n",
        "    Args:\n",
        "        run_dir: Run directory.\n",
        "        device: Torch device used for model evaluation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple `(cfg, model, score_fn)` where score_fn is graph-free.\n",
        "\n",
        "    How it works:\n",
        "        Uses `config_resolved.yaml` + latest checkpoint, applies EMA\n",
        "        shadow weights when available, and returns unified score wrapper.\n",
        "    \"\"\"\n",
        "    cfg = load_run_cfg(run_dir)\n",
        "    model_id = resolve_model_id(cfg)\n",
        "\n",
        "    model = build_model(cfg).to(device)\n",
        "    ckpt_path = latest_checkpoint(run_dir)\n",
        "    if ckpt_path is None:\n",
        "        raise FileNotFoundError(f'No checkpoint found in {run_dir}')\n",
        "\n",
        "    ckpt = load_checkpoint(ckpt_path, map_location=str(device))\n",
        "    model.load_state_dict(ckpt['model'], strict=True)\n",
        "\n",
        "    # EMA shadow가 있으면 평가에는 EMA를 우선 사용한다.\n",
        "    ema = ckpt.get('ema')\n",
        "    if isinstance(ema, dict) and 'shadow' in ema:\n",
        "        model.load_state_dict(ema['shadow'], strict=False)\n",
        "\n",
        "    model.eval()\n",
        "    score_fn = score_fn_from_model(model, model_id, create_graph=False)\n",
        "    return cfg, model, score_fn\n",
        "\n",
        "\n",
        "def evaluate_score_grid(\n",
        "    score_fn,\n",
        "    sigma_value: float,\n",
        "    xlim: tuple[float, float] = (-6.0, 6.0),\n",
        "    ylim: tuple[float, float] = (-6.0, 6.0),\n",
        "    points: int = 35,\n",
        "    device: torch.device = torch.device('cpu'),\n",
        "):\n",
        "    \"\"\"Evaluate 2D score field on a dense grid.\n",
        "\n",
        "    Args:\n",
        "        score_fn: Callable `score_fn(x, sigma)`.\n",
        "        sigma_value: Scalar sigma used for entire grid.\n",
        "        xlim: X-axis range.\n",
        "        ylim: Y-axis range.\n",
        "        points: Number of grid points per axis.\n",
        "        device: Execution device.\n",
        "\n",
        "    Returns:\n",
        "        Tuple `(X, Y, U, V)` for quiver plotting.\n",
        "    \"\"\"\n",
        "    xs = np.linspace(xlim[0], xlim[1], points)\n",
        "    ys = np.linspace(ylim[0], ylim[1], points)\n",
        "    X, Y = np.meshgrid(xs, ys)\n",
        "\n",
        "    grid = np.stack([X.reshape(-1), Y.reshape(-1)], axis=1)\n",
        "    x_tensor = torch.tensor(grid, dtype=torch.float32, device=device)\n",
        "    sigma = torch.full((x_tensor.shape[0],), float(sigma_value), dtype=torch.float32, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        score = score_fn(x_tensor, sigma).detach().cpu().numpy()\n",
        "\n",
        "    U = score[:, 0].reshape(points, points)\n",
        "    V = score[:, 1].reshape(points, points)\n",
        "    return X, Y, U, V\n",
        "\n",
        "\n",
        "def finite_diff_curl(U: np.ndarray, V: np.ndarray, xs: np.ndarray, ys: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute 2D curl proxy `dV/dx - dU/dy` via finite differences.\n",
        "\n",
        "    Args:\n",
        "        U: X-component field array.\n",
        "        V: Y-component field array.\n",
        "        xs: X-axis coordinates.\n",
        "        ys: Y-axis coordinates.\n",
        "\n",
        "    Returns:\n",
        "        Curl array with same shape as `U`/`V`.\n",
        "    \"\"\"\n",
        "    dV_dy, dV_dx = np.gradient(V, ys, xs, edge_order=2)\n",
        "    dU_dy, dU_dx = np.gradient(U, ys, xs, edge_order=2)\n",
        "    return dV_dx - dU_dy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "toy_runs = find_toy_run_dirs(RUN_ROOT, seed_name='seed0')\n",
        "print('Detected toy runs:')\n",
        "for model_id in MODEL_ORDER:\n",
        "    print(f'  {model_id}: {toy_runs.get(model_id)}')\n",
        "\n",
        "# 필요 시 True로 바꿔 eval csv를 자동 생성한다.\n",
        "RUN_EVAL_IF_MISSING = False\n",
        "if RUN_EVAL_IF_MISSING:\n",
        "    for run_dir in toy_runs.values():\n",
        "        maybe_run_eval(run_dir, nfe_list='10,20,50,100,200')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 1) Training Curves\n",
        "# ============================\n",
        "\n",
        "train_frames = []\n",
        "for model_id, run_dir in toy_runs.items():\n",
        "    df = read_csv_if_exists(run_dir / 'metrics.csv')\n",
        "    if df.empty:\n",
        "        continue\n",
        "    df['model_id'] = model_id\n",
        "    train_frames.append(df)\n",
        "\n",
        "train_df = pd.concat(train_frames, ignore_index=True) if train_frames else pd.DataFrame()\n",
        "display(train_df.head())\n",
        "\n",
        "if not train_df.empty:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    for model_id in MODEL_ORDER:\n",
        "        sub = train_df[train_df['model_id'] == model_id]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "\n",
        "        axes[0].plot(sub['step'], sub['loss_total'], label=model_id, linewidth=1.6)\n",
        "        axes[1].plot(sub['step'], sub.get('loss_dsm', np.nan), label=model_id, linewidth=1.6)\n",
        "\n",
        "        reg_total = np.zeros(len(sub), dtype=float)\n",
        "        for col in ['loss_sym', 'loss_loop', 'loss_loop_multi', 'loss_cycle', 'loss_match']:\n",
        "            if col in sub.columns:\n",
        "                reg_total = reg_total + sub[col].fillna(0.0).to_numpy()\n",
        "        axes[2].plot(sub['step'], reg_total, label=model_id, linewidth=1.6)\n",
        "\n",
        "    axes[0].set_title('Total Loss')\n",
        "    axes[1].set_title('DSM Loss')\n",
        "    axes[2].set_title('Regularizer Sum')\n",
        "    for ax in axes:\n",
        "        ax.set_xlabel('step')\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('metrics.csv not found for detected runs.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 2) FID vs NFE\n",
        "# ============================\n",
        "\n",
        "fid_frames = []\n",
        "for model_id, run_dir in toy_runs.items():\n",
        "    df = read_csv_if_exists(run_dir / 'eval' / 'fid_vs_nfe.csv')\n",
        "    if df.empty:\n",
        "        continue\n",
        "    df['model_id'] = model_id\n",
        "    fid_frames.append(df)\n",
        "\n",
        "fid_df = pd.concat(fid_frames, ignore_index=True) if fid_frames else pd.DataFrame()\n",
        "display(fid_df.head())\n",
        "\n",
        "if not fid_df.empty:\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    for model_id in MODEL_ORDER:\n",
        "        sub = fid_df[(fid_df['model_id'] == model_id) & (fid_df['sampler'] == fid_df['sampler'].iloc[0])]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        sub = sub.sort_values('nfe')\n",
        "        ax.plot(sub['nfe'], sub['fid'], marker='o', label=model_id)\n",
        "\n",
        "    ax.set_title('Toy FID vs NFE')\n",
        "    ax.set_xlabel('NFE')\n",
        "    ax.set_ylabel('FID')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('eval/fid_vs_nfe.csv not found. Set RUN_EVAL_IF_MISSING=True and run again.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 3) Integrability vs Sigma\n",
        "# ============================\n",
        "\n",
        "def normalize_integrability_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Normalize legacy/new integrability CSV formats to long schema.\n",
        "\n",
        "    Args:\n",
        "        df: Raw integrability DataFrame from eval output.\n",
        "\n",
        "    Returns:\n",
        "        Long-format DataFrame with columns including\n",
        "        `metric_name`, `sigma_lo`, `sigma_hi`, `value`.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    if 'metric_name' in df.columns and 'value' in df.columns:\n",
        "        return df.copy()\n",
        "\n",
        "    # 레거시 wide-format(r_sym, r_loop)을 long-format으로 변환한다.\n",
        "    rows = []\n",
        "    for metric_col, metric_name in [('r_sym', 'r_sym'), ('r_loop', 'r_loop_multi_total')]:\n",
        "        if metric_col not in df.columns:\n",
        "            continue\n",
        "        tmp = df[['bin', 'sigma_lo', 'sigma_hi', metric_col]].copy()\n",
        "        tmp['count'] = df.get('count', np.nan)\n",
        "        tmp['metric_name'] = metric_name\n",
        "        tmp['scale_delta'] = ''\n",
        "        tmp['cycle_len'] = ''\n",
        "        tmp['value'] = tmp[metric_col]\n",
        "        rows.append(tmp[['bin', 'sigma_lo', 'sigma_hi', 'count', 'metric_name', 'scale_delta', 'cycle_len', 'value']])\n",
        "\n",
        "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
        "\n",
        "\n",
        "integ_frames = []\n",
        "for model_id, run_dir in toy_runs.items():\n",
        "    raw = read_csv_if_exists(run_dir / 'eval' / 'integrability_vs_sigma.csv')\n",
        "    if raw.empty:\n",
        "        continue\n",
        "    df = normalize_integrability_df(raw)\n",
        "    if df.empty:\n",
        "        continue\n",
        "    df['model_id'] = model_id\n",
        "    df['sigma_mid'] = np.sqrt(df['sigma_lo'].astype(float) * df['sigma_hi'].astype(float))\n",
        "    integ_frames.append(df)\n",
        "\n",
        "integ_df = pd.concat(integ_frames, ignore_index=True) if integ_frames else pd.DataFrame()\n",
        "display(integ_df.head())\n",
        "\n",
        "if not integ_df.empty:\n",
        "    metrics_to_plot = ['r_sym', 'r_loop_multi_total', 'r_cycle_total']\n",
        "    fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(6 * len(metrics_to_plot), 4), sharex=True)\n",
        "    if len(metrics_to_plot) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, metric_name in zip(axes, metrics_to_plot):\n",
        "        sub_all = integ_df[integ_df['metric_name'] == metric_name]\n",
        "        if sub_all.empty:\n",
        "            ax.set_title(f'{metric_name} (no data)')\n",
        "            continue\n",
        "\n",
        "        for model_id in MODEL_ORDER:\n",
        "            sub = sub_all[sub_all['model_id'] == model_id].sort_values('sigma_mid')\n",
        "            if sub.empty:\n",
        "                continue\n",
        "            ax.plot(sub['sigma_mid'], sub['value'], marker='o', label=model_id)\n",
        "\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_title(metric_name)\n",
        "        ax.set_xlabel('sigma (bin midpoint)')\n",
        "\n",
        "    axes[0].set_ylabel('value')\n",
        "    axes[0].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('eval/integrability_vs_sigma.csv not found. Set RUN_EVAL_IF_MISSING=True and run again.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 4) Vector Field + Curl Heatmap\n",
        "# ============================\n",
        "\n",
        "SIGMA_VIS = 0.20\n",
        "GRID_POINTS = 31\n",
        "\n",
        "available_models = [m for m in MODEL_ORDER if m in toy_runs]\n",
        "if not available_models:\n",
        "    print('No toy runs detected.')\n",
        "else:\n",
        "    fig, axes = plt.subplots(len(available_models), 2, figsize=(12, 5 * len(available_models)))\n",
        "    if len(available_models) == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for row_idx, model_id in enumerate(available_models):\n",
        "        run_dir = toy_runs[model_id]\n",
        "        cfg, model, score_fn = load_model_score_fn(run_dir, DEVICE)\n",
        "\n",
        "        X, Y, U, V = evaluate_score_grid(\n",
        "            score_fn=score_fn,\n",
        "            sigma_value=SIGMA_VIS,\n",
        "            xlim=(-6, 6),\n",
        "            ylim=(-6, 6),\n",
        "            points=GRID_POINTS,\n",
        "            device=DEVICE,\n",
        "        )\n",
        "\n",
        "        xs = np.linspace(-6, 6, GRID_POINTS)\n",
        "        ys = np.linspace(-6, 6, GRID_POINTS)\n",
        "        curl = finite_diff_curl(U, V, xs=xs, ys=ys)\n",
        "\n",
        "        ax_field = axes[row_idx, 0]\n",
        "        ax_field.quiver(X, Y, U, V, angles='xy', scale_units='xy', scale=10)\n",
        "        ax_field.set_title(f'{model_id} score field @ sigma={SIGMA_VIS}')\n",
        "        ax_field.set_xlim(-6, 6)\n",
        "        ax_field.set_ylim(-6, 6)\n",
        "        ax_field.set_aspect('equal')\n",
        "\n",
        "        ax_curl = axes[row_idx, 1]\n",
        "        im = ax_curl.imshow(curl, extent=[-6, 6, -6, 6], origin='lower', cmap='coolwarm')\n",
        "        ax_curl.set_title(f'{model_id} curl proxy')\n",
        "        ax_curl.set_aspect('equal')\n",
        "        fig.colorbar(im, ax=ax_curl, fraction=0.046, pad=0.04)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# 5) Sampling Trajectories (Heun)\n",
        "# ============================\n",
        "\n",
        "N_TRAJ = 16\n",
        "NFE = 50\n",
        "RADIUS = 5.0\n",
        "\n",
        "angles = np.linspace(0, 2 * np.pi, N_TRAJ, endpoint=False)\n",
        "init_np = np.stack([RADIUS * np.cos(angles), RADIUS * np.sin(angles)], axis=1).astype(np.float32)\n",
        "init_x = torch.from_numpy(init_np).to(DEVICE)\n",
        "\n",
        "available_models = [m for m in MODEL_ORDER if m in toy_runs]\n",
        "if not available_models:\n",
        "    print('No toy runs detected.')\n",
        "else:\n",
        "    fig, axes = plt.subplots(1, len(available_models), figsize=(5 * len(available_models), 5), sharex=True, sharey=True)\n",
        "    if len(available_models) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, model_id in zip(axes, available_models):\n",
        "        run_dir = toy_runs[model_id]\n",
        "        cfg, model, score_fn = load_model_score_fn(run_dir, DEVICE)\n",
        "        sigma_min = float(cfg['loss']['sigma_min'])\n",
        "        sigma_max = float(cfg['loss']['sigma_max'])\n",
        "\n",
        "        score_requires_grad = model_id in {'M2', 'M4'}\n",
        "        _, stats = sample_heun(\n",
        "            score_fn=score_fn,\n",
        "            shape=tuple(init_x.shape),\n",
        "            sigma_min=sigma_min,\n",
        "            sigma_max=sigma_max,\n",
        "            nfe=NFE,\n",
        "            device=DEVICE,\n",
        "            init_x=init_x.clone(),\n",
        "            score_requires_grad=score_requires_grad,\n",
        "            return_trajectory=True,\n",
        "        )\n",
        "\n",
        "        traj = [t.detach().cpu().numpy() for t in stats['trajectory']]\n",
        "        traj_arr = np.stack(traj, axis=0)  # [T, B, 2]\n",
        "\n",
        "        for i in range(traj_arr.shape[1]):\n",
        "            ax.plot(traj_arr[:, i, 0], traj_arr[:, i, 1], linewidth=1.0, alpha=0.8)\n",
        "\n",
        "        ax.scatter(traj_arr[0, :, 0], traj_arr[0, :, 1], s=20, c='black', marker='x', label='start')\n",
        "        ax.scatter(traj_arr[-1, :, 0], traj_arr[-1, :, 1], s=20, c='red', marker='o', label='end')\n",
        "        ax.set_title(f'{model_id} trajectories')\n",
        "        ax.set_aspect('equal')\n",
        "        ax.set_xlim(-6, 6)\n",
        "        ax.set_ylim(-6, 6)\n",
        "        ax.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 사용 팁\n",
        "- `RUN_EVAL_IF_MISSING=True`로 바꾸면 eval CSV가 없는 런에 대해 자동으로 `main_eval`을 실행합니다.\n",
        "- `SIGMA_VIS`, `GRID_POINTS`, `NFE`를 바꿔서 해상도/속도 균형을 조절할 수 있습니다.\n",
        "- 새 실험 경로(`runs/toy/M3/seed1` 등)를 보고 싶으면 `find_toy_run_dirs(..., seed_name='seed1')`로 변경하세요.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}