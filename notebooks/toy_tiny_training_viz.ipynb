{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b45e122c",
      "metadata": {},
      "source": [
        "# Toy Tiny Training Loop Visualizer\n",
        "\n",
        "이 노트북은 **아주 작은 학습 루프**를 M0~M4에 대해 직접 돌리고,\n",
        "각 epoch마다 시각화해서 **학습 속도**와 **학습 품질**을 비교하는 용도입니다.\n",
        "\n",
        "비교 항목:\n",
        "- 학습 속도: epoch 시간, step 시간\n",
        "- 학습 품질: validation DSM, real-nearest distance(작을수록 좋음)\n",
        "- epoch별 샘플 분포(실데이터 오버레이)\n",
        "- epoch별 벡터장 + curl 히트맵\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "90cb6de2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROOT=/home/junyeobe/projects/Advance_score\n",
            "DEVICE=cuda\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import time\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "def resolve_repo_root(start: Path) -> Path:\n",
        "    \"\"\"Resolve repository root by searching parent directories.\n",
        "\n",
        "    Args:\n",
        "        start: Starting path (notebook working directory).\n",
        "\n",
        "    Returns:\n",
        "        Repo root path containing `src/` and `configs/`.\n",
        "\n",
        "    How it works:\n",
        "        Walks upward from `start` to filesystem root and returns the\n",
        "        first directory that matches this project layout.\n",
        "    \"\"\"\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if (candidate / 'src').is_dir() and (candidate / 'configs').is_dir():\n",
        "            return candidate\n",
        "    raise RuntimeError(\n",
        "        f'Could not locate repo root from {start}. '\n",
        "        'Open this notebook inside the Advance_score repository.'\n",
        "    )\n",
        "\n",
        "\n",
        "ROOT = resolve_repo_root(Path.cwd().resolve())\n",
        "if str(ROOT) not in sys.path:\n",
        "    # 로컬 src 패키지 임포트를 위해 루트를 path에 추가한다.\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from src.data import make_loader, sample_toy_data, unpack_batch\n",
        "from src.models import build_model, score_fn_from_model\n",
        "from src.sampling import sample_heun\n",
        "from src.trainers.train_step_baseline import train_step_baseline\n",
        "from src.trainers.train_step_m3 import train_step_m3\n",
        "from src.trainers.train_step_m4 import train_step_m4\n",
        "from src.trainers.train_step_reg import train_step_reg\n",
        "from src.trainers.train_step_struct import train_step_struct\n",
        "from src.trainers.common import compute_dsm_for_score\n",
        "from src.utils.config import ensure_experiment_defaults, load_config\n",
        "from src.utils.feature_encoder import build_feature_encoder\n",
        "from src.utils.seed import seed_everything\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'ROOT={ROOT}')\n",
        "print(f'DEVICE={DEVICE}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4f8d473e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configured tiny training loop.\n"
          ]
        }
      ],
      "source": [
        "# ===== Tiny Training Controls =====\n",
        "MODEL_IDS = ['M0', 'M1', 'M2', 'M3', 'M4']\n",
        "SEED = 0\n",
        "\n",
        "EPOCHS = 10\n",
        "STEPS_PER_EPOCH = 30\n",
        "BATCH_SIZE = 512\n",
        "LR = 2.0e-4\n",
        "\n",
        "NUM_EVAL_REAL = 1024\n",
        "NUM_EVAL_FAKE = 1024\n",
        "EVAL_DSM_REPEATS = 3\n",
        "SAMPLE_NFE = 40\n",
        "\n",
        "SIGMA_VIS = 0.20\n",
        "GRID_POINTS = 27\n",
        "\n",
        "# 시각화할 epoch 목록. None이면 모든 epoch를 표시한다.\n",
        "EPOCHS_TO_SHOW = None\n",
        "\n",
        "print('Configured tiny training loop.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper/Trainer 셀 설명 (중요)\n",
        "\n",
        "아래 코드 셀은 **tiny 학습 실험의 핵심 로직**을 담고 있습니다.\n",
        "\n",
        "무엇을 하나요:\n",
        "- `model_id -> config` 매핑(`M0~M4`)\n",
        "- 모델별 학습 step dispatch (`run_one_train_step`)\n",
        "- epoch 단위 평가(`val_dsm`, `nn_real_dist`, 샘플, 벡터장, curl)\n",
        "- 전체 tiny 학습 루프 실행(`run_tiny_training_suite`)\n",
        "\n",
        "모델별 objective(loss) 정리:\n",
        "- `M0` (Baseline): `L = L_DSM`\n",
        "- `M1` (Jacobian reg): `L = L_DSM + lambda_sym * R_sym + mu_loop * R_loop`\n",
        "- `M2` (Struct conservative): `s = grad_x phi`, `L = L_DSM`\n",
        "- `M3` (Jacobian-free nonlocal): `L = L_DSM + mu1 * R_loop_multi + mu2 * R_cycle`\n",
        "- `M4` (Hybrid hard-conservative): `L = L_DSM + alpha * R_match + beta * R_cycle_low(optional)`\n",
        "\n",
        "로그 지표 해석:\n",
        "- `train_loss_mean`: epoch 내 평균 학습 loss (낮을수록 좋음)\n",
        "- `val_dsm`: 고정 eval batch에서 DSM 추정 (낮을수록 좋음)\n",
        "- `nn_real_dist`: 생성 샘플의 최근접 real 거리 평균 (낮을수록 분포 적합)\n",
        "- `step_time_ms_mean`: step당 시간 (낮을수록 빠름)\n",
        "- `trajectory_length_mean`: 샘플링 궤적 길이 (동역학 비교 지표)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1a199438",
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "f-string: unmatched '[' (4291461890.py, line 397)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 397\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mf'val_dsm={eval_out['val_dsm']:.4f}, '\u001b[39m\n                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m f-string: unmatched '['\n"
          ]
        }
      ],
      "source": [
        "def model_id_to_config_path(model_id: str) -> Path:\n",
        "    \"\"\"Map canonical model id to toy config path.\n",
        "\n",
        "    Args:\n",
        "        model_id: Canonical model token (`M0`..`M4`).\n",
        "\n",
        "    Returns:\n",
        "        Config file path under `configs/toy`.\n",
        "    \"\"\"\n",
        "    suffix = model_id[1:]\n",
        "    return ROOT / 'configs' / 'toy' / f'm{suffix}.yaml'\n",
        "\n",
        "\n",
        "def load_tiny_cfg(model_id: str, batch_size: int, lr: float) -> dict:\n",
        "    \"\"\"Load and override toy config for tiny-loop experiments.\n",
        "\n",
        "    Args:\n",
        "        model_id: Canonical model token (`M0`..`M4`).\n",
        "        batch_size: Tiny-loop training batch size.\n",
        "        lr: Learning rate for optimizer.\n",
        "\n",
        "    Returns:\n",
        "        Resolved config dictionary tailored for tiny interactive runs.\n",
        "\n",
        "    How it works:\n",
        "        Starts from official toy config and applies minimal overrides\n",
        "        so training stays fast in notebook mode.\n",
        "    \"\"\"\n",
        "    cfg = ensure_experiment_defaults(load_config(model_id_to_config_path(model_id)))\n",
        "\n",
        "    # Tiny-loop overrides for fast interactive iteration.\n",
        "    cfg['dataset']['batch_size'] = int(batch_size)\n",
        "    cfg['dataset']['num_workers'] = 0\n",
        "    cfg['train']['amp'] = False\n",
        "    cfg['train']['lr'] = float(lr)\n",
        "    cfg['train']['clip_grad_norm'] = 1.0\n",
        "    cfg['loss']['reg_freq'] = 1\n",
        "\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def score_requires_grad(model_id: str) -> bool:\n",
        "    \"\"\"Return whether sampler must enable grad path for this model id.\n",
        "\n",
        "    Args:\n",
        "        model_id: Canonical model token.\n",
        "\n",
        "    Returns:\n",
        "        True for structure-dependent score wrappers (`M2`, `M4`).\n",
        "    \"\"\"\n",
        "    return model_id in {'M2', 'M4'}\n",
        "\n",
        "\n",
        "def run_one_train_step(\n",
        "    model_id: str,\n",
        "    model: torch.nn.Module,\n",
        "    x0: torch.Tensor,\n",
        "    cfg: dict,\n",
        "    global_step: int,\n",
        "    feature_encoder: torch.nn.Module | None,\n",
        ") -> tuple[torch.Tensor, dict[str, float]]:\n",
        "    \"\"\"Dispatch one training objective step by model id.\n",
        "\n",
        "    Args:\n",
        "        model_id: Canonical model token (`M0`..`M4`).\n",
        "        model: Active model instance.\n",
        "        x0: Clean training batch.\n",
        "        cfg: Model config dictionary.\n",
        "        global_step: Global step index used by gated regularizers.\n",
        "        feature_encoder: Optional frozen encoder for M3/M4 cycle losses.\n",
        "\n",
        "    Returns:\n",
        "        Tuple `(loss_tensor, metric_dict)`.\n",
        "\n",
        "    Model-Loss mapping:\n",
        "        - M0: DSM only\n",
        "        - M1: DSM + Jacobian asymmetry/loop regularization\n",
        "        - M2: DSM with conservative structure (`score = grad phi`)\n",
        "        - M3: DSM + multi-scale loop + graph cycle\n",
        "        - M4: DSM + boundary matching + optional low-noise cycle\n",
        "    \"\"\"\n",
        "    if model_id == 'M0':\n",
        "        return train_step_baseline(model, x0, cfg)\n",
        "    if model_id == 'M1':\n",
        "        return train_step_reg(model, x0, cfg, global_step)\n",
        "    if model_id == 'M2':\n",
        "        return train_step_struct(model, x0, cfg)\n",
        "    if model_id == 'M3':\n",
        "        assert feature_encoder is not None\n",
        "        return train_step_m3(model, x0, cfg, global_step, feature_encoder)\n",
        "    if model_id == 'M4':\n",
        "        assert feature_encoder is not None\n",
        "        return train_step_m4(model, x0, cfg, global_step, feature_encoder)\n",
        "\n",
        "    raise ValueError(f'Unsupported model_id: {model_id}')\n",
        "\n",
        "\n",
        "def evaluate_score_grid(\n",
        "    score_fn,\n",
        "    sigma_value: float,\n",
        "    points: int,\n",
        "    xlim: tuple[float, float] = (-6.0, 6.0),\n",
        "    ylim: tuple[float, float] = (-6.0, 6.0),\n",
        "    device: torch.device = torch.device('cpu'),\n",
        ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Evaluate score field on a dense 2D grid for visualization.\n",
        "\n",
        "    Args:\n",
        "        score_fn: Callable `score_fn(x, sigma)` returning `[B,2]` score.\n",
        "        sigma_value: Scalar sigma used for entire grid.\n",
        "        points: Number of grid points per axis.\n",
        "        xlim: X-axis range.\n",
        "        ylim: Y-axis range.\n",
        "        device: Execution device for score evaluation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple `(X, Y, U, V)` for vector-field plotting.\n",
        "    \"\"\"\n",
        "    xs = np.linspace(xlim[0], xlim[1], points)\n",
        "    ys = np.linspace(ylim[0], ylim[1], points)\n",
        "    X, Y = np.meshgrid(xs, ys)\n",
        "\n",
        "    grid_np = np.stack([X.reshape(-1), Y.reshape(-1)], axis=1).astype(np.float32)\n",
        "    x_tensor = torch.from_numpy(grid_np).to(device)\n",
        "    sigma = torch.full((x_tensor.shape[0],), float(sigma_value), device=device, dtype=torch.float32)\n",
        "\n",
        "    with torch.enable_grad():\n",
        "        score = score_fn(x_tensor, sigma).detach().cpu().numpy()\n",
        "\n",
        "    U = score[:, 0].reshape(points, points)\n",
        "    V = score[:, 1].reshape(points, points)\n",
        "    return X, Y, U, V\n",
        "\n",
        "\n",
        "def finite_diff_curl(U: np.ndarray, V: np.ndarray, xs: np.ndarray, ys: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute finite-difference curl proxy `dV/dx - dU/dy` in 2D.\n",
        "\n",
        "    Args:\n",
        "        U: X-component grid array.\n",
        "        V: Y-component grid array.\n",
        "        xs: X-axis coordinates.\n",
        "        ys: Y-axis coordinates.\n",
        "\n",
        "    Returns:\n",
        "        Curl array with same shape as `U`/`V`.\n",
        "    \"\"\"\n",
        "    dV_dy, dV_dx = np.gradient(V, ys, xs, edge_order=2)\n",
        "    dU_dy, dU_dx = np.gradient(U, ys, xs, edge_order=2)\n",
        "    return dV_dx - dU_dy\n",
        "\n",
        "\n",
        "def evaluate_model_epoch(\n",
        "    model_id: str,\n",
        "    model: torch.nn.Module,\n",
        "    cfg: dict,\n",
        "    real_eval: torch.Tensor,\n",
        "    eval_dsm_repeats: int,\n",
        "    num_eval_fake: int,\n",
        "    sample_nfe: int,\n",
        "    sigma_vis: float,\n",
        "    grid_points: int,\n",
        "    device: torch.device,\n",
        ") -> dict:\n",
        "    \"\"\"Compute epoch-level quality/speed diagnostics for one model.\n",
        "\n",
        "    Args:\n",
        "        model_id: Canonical model token (`M0`..`M4`).\n",
        "        model: Trained model at current epoch.\n",
        "        cfg: Model config dictionary.\n",
        "        real_eval: Fixed real toy batch used for epoch-wise eval.\n",
        "        eval_dsm_repeats: Number of DSM Monte-Carlo repeats.\n",
        "        num_eval_fake: Number of generated fake samples for comparison.\n",
        "        sample_nfe: NFE for Heun sampler.\n",
        "        sigma_vis: Sigma value for field visualization snapshots.\n",
        "        grid_points: Field-grid resolution per axis.\n",
        "        device: Runtime device.\n",
        "\n",
        "    Returns:\n",
        "        Dict with keys:\n",
        "        - `val_dsm`: validation DSM estimate\n",
        "        - `nn_real_dist`: mean nearest-real distance\n",
        "        - `trajectory_length_mean`: sampler trajectory length\n",
        "        - `samples_np`: generated samples numpy array\n",
        "        - `field`: dict containing X/Y/U/V/curl arrays\n",
        "\n",
        "    How it works:\n",
        "        Reuses project score wrapper + Heun sampler, then computes a simple\n",
        "        distribution-fit proxy (`nearest real distance`) and field snapshot.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    sigma_min = float(cfg['loss']['sigma_min'])\n",
        "    sigma_max = float(cfg['loss']['sigma_max'])\n",
        "    weight_mode = str(cfg['loss'].get('weight_mode', 'sigma2'))\n",
        "\n",
        "    score_fn = score_fn_from_model(model, model_id, create_graph=False)\n",
        "\n",
        "    dsm_vals = []\n",
        "    for _ in range(int(eval_dsm_repeats)):\n",
        "        dsm, _ = compute_dsm_for_score(\n",
        "            score_fn=score_fn,\n",
        "            x0=real_eval,\n",
        "            sigma_min=sigma_min,\n",
        "            sigma_max=sigma_max,\n",
        "            weight_mode=weight_mode,\n",
        "        )\n",
        "        dsm_vals.append(float(dsm.detach().item()))\n",
        "    val_dsm = float(np.mean(dsm_vals))\n",
        "\n",
        "    fake, stats = sample_heun(\n",
        "        score_fn=score_fn,\n",
        "        shape=(int(num_eval_fake), 2),\n",
        "        sigma_min=sigma_min,\n",
        "        sigma_max=sigma_max,\n",
        "        nfe=int(sample_nfe),\n",
        "        device=device,\n",
        "        score_requires_grad=score_requires_grad(model_id),\n",
        "        return_trajectory=False,\n",
        "    )\n",
        "\n",
        "    fake_detached = fake.detach()\n",
        "    real_for_dist = real_eval[: fake_detached.shape[0]]\n",
        "    nn_dist = torch.cdist(fake_detached, real_for_dist).min(dim=1).values.mean().item()\n",
        "\n",
        "    X, Y, U, V = evaluate_score_grid(\n",
        "        score_fn=score_fn,\n",
        "        sigma_value=float(sigma_vis),\n",
        "        points=int(grid_points),\n",
        "        device=device,\n",
        "    )\n",
        "    xs = np.linspace(-6.0, 6.0, int(grid_points))\n",
        "    ys = np.linspace(-6.0, 6.0, int(grid_points))\n",
        "    curl = finite_diff_curl(U=U, V=V, xs=xs, ys=ys)\n",
        "\n",
        "    return {\n",
        "        'val_dsm': float(val_dsm),\n",
        "        'nn_real_dist': float(nn_dist),\n",
        "        'trajectory_length_mean': float(stats.get('trajectory_length_mean', float('nan'))),\n",
        "        'samples_np': fake_detached.detach().cpu().numpy(),\n",
        "        'field': {'X': X, 'Y': Y, 'U': U, 'V': V, 'curl': curl},\n",
        "    }\n",
        "\n",
        "\n",
        "def run_tiny_training_suite(\n",
        "    model_ids: list[str],\n",
        "    epochs: int,\n",
        "    steps_per_epoch: int,\n",
        "    batch_size: int,\n",
        "    lr: float,\n",
        "    num_eval_real: int,\n",
        "    num_eval_fake: int,\n",
        "    eval_dsm_repeats: int,\n",
        "    sample_nfe: int,\n",
        "    sigma_vis: float,\n",
        "    grid_points: int,\n",
        "    device: torch.device,\n",
        "    seed: int,\n",
        "):\n",
        "    \"\"\"Run tiny training loops for multiple model ids and collect snapshots.\n",
        "\n",
        "    Args:\n",
        "        model_ids: Model ids to train (`M0`..`M4`).\n",
        "        epochs: Number of tiny-loop epochs.\n",
        "        steps_per_epoch: Number of optimizer steps per epoch.\n",
        "        batch_size: Training batch size.\n",
        "        lr: AdamW learning rate.\n",
        "        num_eval_real: Number of fixed real eval samples.\n",
        "        num_eval_fake: Number of generated eval samples per epoch.\n",
        "        eval_dsm_repeats: DSM repeat count for smoother estimate.\n",
        "        sample_nfe: Heun NFE for epoch eval.\n",
        "        sigma_vis: Sigma for vector-field snapshots.\n",
        "        grid_points: Grid resolution for vector-field snapshots.\n",
        "        device: Runtime torch device.\n",
        "        seed: Global random seed.\n",
        "\n",
        "    Returns:\n",
        "        Tuple `(history_df, sample_snaps, field_snaps, real_eval_np)`.\n",
        "\n",
        "    How it works:\n",
        "        Initializes one model/optimizer/loader per model id, trains each\n",
        "        for `steps_per_epoch`, then logs epoch metrics + visual snapshots.\n",
        "    \"\"\"\n",
        "    seed_everything(int(seed))\n",
        "\n",
        "    states = {}\n",
        "    for model_id in model_ids:\n",
        "        cfg = load_tiny_cfg(model_id=model_id, batch_size=batch_size, lr=lr)\n",
        "\n",
        "        model = build_model(cfg).to(device)\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=float(cfg['train']['lr']),\n",
        "            betas=tuple(cfg['train']['betas']),\n",
        "            weight_decay=float(cfg['train'].get('weight_decay', 0.01)),\n",
        "        )\n",
        "\n",
        "        loader = make_loader(cfg, train=True)\n",
        "        data_iter = iter(loader)\n",
        "\n",
        "        feature_encoder = None\n",
        "        if model_id in {'M3', 'M4'}:\n",
        "            feature_encoder = build_feature_encoder(\n",
        "                dataset_name='toy',\n",
        "                channels=int(cfg['dataset'].get('channels', 1)),\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "        states[model_id] = {\n",
        "            'cfg': cfg,\n",
        "            'model': model,\n",
        "            'optimizer': optimizer,\n",
        "            'loader': loader,\n",
        "            'data_iter': data_iter,\n",
        "            'feature_encoder': feature_encoder,\n",
        "            'global_step': 0,\n",
        "        }\n",
        "\n",
        "    # 고정 평가 배치: epoch 간 비교 분산을 줄이기 위해 공유한다.\n",
        "    cfg_ref = states[model_ids[0]]['cfg']\n",
        "    real_eval = sample_toy_data(cfg_ref, num_samples=int(num_eval_real)).to(device)\n",
        "\n",
        "    history_rows = []\n",
        "    sample_snaps = {m: [] for m in model_ids}\n",
        "    field_snaps = {m: [] for m in model_ids}\n",
        "\n",
        "    for epoch in range(1, int(epochs) + 1):\n",
        "        print(f'\\n[Epoch {epoch}/{epochs}]')\n",
        "        for model_id in model_ids:\n",
        "            state = states[model_id]\n",
        "            cfg = state['cfg']\n",
        "            model = state['model']\n",
        "            optimizer = state['optimizer']\n",
        "            feature_encoder = state['feature_encoder']\n",
        "\n",
        "            model.train()\n",
        "            step_times = []\n",
        "            train_loss_vals = []\n",
        "\n",
        "            epoch_t0 = time.perf_counter()\n",
        "            for _ in range(int(steps_per_epoch)):\n",
        "                state['global_step'] += 1\n",
        "\n",
        "                try:\n",
        "                    batch = next(state['data_iter'])\n",
        "                except StopIteration:\n",
        "                    state['data_iter'] = iter(state['loader'])\n",
        "                    batch = next(state['data_iter'])\n",
        "\n",
        "                x0 = unpack_batch(batch).to(device)\n",
        "\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                t0 = time.perf_counter()\n",
        "\n",
        "                loss, metrics = run_one_train_step(\n",
        "                    model_id=model_id,\n",
        "                    model=model,\n",
        "                    x0=x0,\n",
        "                    cfg=cfg,\n",
        "                    global_step=state['global_step'],\n",
        "                    feature_encoder=feature_encoder,\n",
        "                )\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(cfg['train']['clip_grad_norm']))\n",
        "                optimizer.step()\n",
        "\n",
        "                step_ms = (time.perf_counter() - t0) * 1000.0\n",
        "                step_times.append(step_ms)\n",
        "                train_loss_vals.append(float(metrics['loss_total']))\n",
        "\n",
        "            epoch_time = time.perf_counter() - epoch_t0\n",
        "\n",
        "            eval_out = evaluate_model_epoch(\n",
        "                model_id=model_id,\n",
        "                model=model,\n",
        "                cfg=cfg,\n",
        "                real_eval=real_eval,\n",
        "                eval_dsm_repeats=eval_dsm_repeats,\n",
        "                num_eval_fake=num_eval_fake,\n",
        "                sample_nfe=sample_nfe,\n",
        "                sigma_vis=sigma_vis,\n",
        "                grid_points=grid_points,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "            sample_snaps[model_id].append(eval_out['samples_np'])\n",
        "            field_snaps[model_id].append(eval_out['field'])\n",
        "\n",
        "            history_rows.append(\n",
        "                {\n",
        "                    'epoch': int(epoch),\n",
        "                    'model_id': model_id,\n",
        "                    'train_loss_mean': float(np.mean(train_loss_vals)),\n",
        "                    'step_time_ms_mean': float(np.mean(step_times)),\n",
        "                    'epoch_time_sec': float(epoch_time),\n",
        "                    'val_dsm': float(eval_out['val_dsm']),\n",
        "                    'nn_real_dist': float(eval_out['nn_real_dist']),\n",
        "                    'trajectory_length_mean': float(eval_out['trajectory_length_mean']),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Epoch 요약: 모델별 수렴 속도(손실)와 품질(proxy)를 함께 출력한다.\n",
        "            print(\n",
        "                f'  {model_id}: '\n",
        "                f'train_loss={np.mean(train_loss_vals):.4f}, '\n",
        "                f'val_dsm={eval_out[\"val_dsm\"]:.4f}, '\n",
        "                f'nn_real_dist={eval_out[\"nn_real_dist\"]:.4f}, '\n",
        "                f'step_ms={np.mean(step_times):.2f}'\n",
        "            )\n",
        "\n",
        "    history_df = pd.DataFrame(history_rows)\n",
        "    return history_df, sample_snaps, field_snaps, real_eval.detach().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86eea4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "history_df, sample_snaps, field_snaps, real_eval_np = run_tiny_training_suite(\n",
        "    model_ids=MODEL_IDS,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=LR,\n",
        "    num_eval_real=NUM_EVAL_REAL,\n",
        "    num_eval_fake=NUM_EVAL_FAKE,\n",
        "    eval_dsm_repeats=EVAL_DSM_REPEATS,\n",
        "    sample_nfe=SAMPLE_NFE,\n",
        "    sigma_vis=SIGMA_VIS,\n",
        "    grid_points=GRID_POINTS,\n",
        "    device=DEVICE,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "display(history_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d6797be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Speed / Quality Curves\n",
        "# ============================\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(22, 4))\n",
        "\n",
        "for model_id in MODEL_IDS:\n",
        "    sub = history_df[history_df['model_id'] == model_id].sort_values('epoch')\n",
        "    if sub.empty:\n",
        "        continue\n",
        "\n",
        "    axes[0].plot(sub['epoch'], sub['train_loss_mean'], marker='o', label=model_id)\n",
        "    axes[1].plot(sub['epoch'], sub['val_dsm'], marker='o', label=model_id)\n",
        "    axes[2].plot(sub['epoch'], sub['nn_real_dist'], marker='o', label=model_id)\n",
        "    axes[3].plot(sub['epoch'], sub['step_time_ms_mean'], marker='o', label=model_id)\n",
        "\n",
        "axes[0].set_title('Train Loss (mean/epoch)')\n",
        "axes[1].set_title('Validation DSM')\n",
        "axes[2].set_title('Nearest Real Distance')\n",
        "axes[3].set_title('Step Time (ms)')\n",
        "\n",
        "for ax in axes:\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e7ab4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_table = (\n",
        "    history_df.sort_values('epoch')\n",
        "    .groupby('model_id', as_index=False)\n",
        "    .tail(1)[['model_id', 'epoch', 'train_loss_mean', 'val_dsm', 'nn_real_dist', 'step_time_ms_mean', 'epoch_time_sec']]\n",
        "    .sort_values(['val_dsm', 'nn_real_dist'])\n",
        ")\n",
        "display(final_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f94dff87",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_epochs_to_show(max_epoch: int, requested: list[int] | None):\n",
        "    \"\"\"Resolve epoch list for visualization loops.\n",
        "\n",
        "    Args:\n",
        "        max_epoch: Maximum available epoch index.\n",
        "        requested: Optional user-requested epoch list.\n",
        "\n",
        "    Returns:\n",
        "        Valid epoch list in ascending order.\n",
        "    \"\"\"\n",
        "    if requested is None:\n",
        "        return list(range(1, max_epoch + 1))\n",
        "    out = sorted(set(int(e) for e in requested if 1 <= int(e) <= max_epoch))\n",
        "    return out if out else [max_epoch]\n",
        "\n",
        "\n",
        "def plot_sample_epoch(epoch: int) -> None:\n",
        "    \"\"\"Plot generated sample scatter per model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        epoch: 1-based epoch index to visualize.\n",
        "\n",
        "    Returns:\n",
        "        None. Displays matplotlib figure.\n",
        "    \"\"\"\n",
        "    idx = int(epoch) - 1\n",
        "    fig, axes = plt.subplots(1, len(MODEL_IDS), figsize=(4.5 * len(MODEL_IDS), 4), sharex=True, sharey=True)\n",
        "    if len(MODEL_IDS) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, model_id in zip(axes, MODEL_IDS):\n",
        "        fake = sample_snaps[model_id][idx]\n",
        "\n",
        "        # 실데이터를 옅은 점으로 깔고 생성 샘플을 덧그린다.\n",
        "        ax.scatter(real_eval_np[:, 0], real_eval_np[:, 1], s=4, alpha=0.18, label='real', color='tab:gray')\n",
        "        ax.scatter(fake[:, 0], fake[:, 1], s=6, alpha=0.6, label='fake', color='tab:blue')\n",
        "\n",
        "        ax.set_title(f'{model_id} - epoch {epoch}')\n",
        "        ax.set_xlim(-6, 6)\n",
        "        ax.set_ylim(-6, 6)\n",
        "        ax.set_aspect('equal')\n",
        "\n",
        "    axes[0].legend(loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "epochs_to_show = get_epochs_to_show(max_epoch=EPOCHS, requested=EPOCHS_TO_SHOW)\n",
        "for epoch in epochs_to_show:\n",
        "    plot_sample_epoch(epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e95ca177",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_field_epoch(epoch: int) -> None:\n",
        "    \"\"\"Plot vector field and curl heatmap per model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        epoch: 1-based epoch index.\n",
        "\n",
        "    Returns:\n",
        "        None. Displays matplotlib figure.\n",
        "    \"\"\"\n",
        "    idx = int(epoch) - 1\n",
        "    fig, axes = plt.subplots(len(MODEL_IDS), 2, figsize=(12, 4.2 * len(MODEL_IDS)))\n",
        "    if len(MODEL_IDS) == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for row, model_id in enumerate(MODEL_IDS):\n",
        "        snap = field_snaps[model_id][idx]\n",
        "        X, Y = snap['X'], snap['Y']\n",
        "        U, V = snap['U'], snap['V']\n",
        "        curl = snap['curl']\n",
        "\n",
        "        ax0 = axes[row, 0]\n",
        "        ax0.quiver(X, Y, U, V, angles='xy', scale_units='xy', scale=10)\n",
        "        ax0.set_title(f'{model_id} field @ epoch {epoch}')\n",
        "        ax0.set_xlim(-6, 6)\n",
        "        ax0.set_ylim(-6, 6)\n",
        "        ax0.set_aspect('equal')\n",
        "\n",
        "        ax1 = axes[row, 1]\n",
        "        im = ax1.imshow(curl, extent=[-6, 6, -6, 6], origin='lower', cmap='coolwarm')\n",
        "        ax1.set_title(f'{model_id} curl @ epoch {epoch}')\n",
        "        ax1.set_aspect('equal')\n",
        "        fig.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for epoch in epochs_to_show:\n",
        "    plot_field_epoch(epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f486ce",
      "metadata": {},
      "source": [
        "## 빠른 사용 가이드\n",
        "- 속도 위주 비교: `STEPS_PER_EPOCH`를 줄이고 `EPOCHS`를 5~8로 둡니다.\n",
        "- 품질 위주 비교: `NUM_EVAL_FAKE`와 `SAMPLE_NFE`를 늘립니다.\n",
        "- GPU 메모리가 부족하면 `BATCH_SIZE`와 `NUM_EVAL_FAKE`를 줄입니다.\n",
        "- 특정 epoch만 보고 싶으면 `EPOCHS_TO_SHOW = [1, 3, 5, EPOCHS]`처럼 지정합니다.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}